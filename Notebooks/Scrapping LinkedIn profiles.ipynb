{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaca246",
   "metadata": {},
   "source": [
    "# Scrapping LinkedIn profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "0d62e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from requests.exceptions import RequestException\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79971285",
   "metadata": {},
   "source": [
    "# Function that scrapes LinkedIn profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "234d6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_profile2(linkedin_profile_url: str):\n",
    "    api_key = \"2ZjHkun0Sq6KGa_o-4p7jg\"\n",
    "    header_dic = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    api_endpoint = \"https://nubela.co/proxycurl/api/v2/linkedin\"\n",
    "\n",
    "    response = requests.get(\n",
    "        api_endpoint, params={\"url\": linkedin_profile_url}, headers=header_dic\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        data = {\n",
    "            k: v\n",
    "            for k, v in data.items()\n",
    "            if v not in ([], \"\", \"\", None)\n",
    "            and k\n",
    "            not in [\n",
    "                \"people_also_viewed\",\n",
    "                \"recommendations\",\n",
    "                \"similarly_named_profiles\",\n",
    "                \"articles\",\n",
    "                \"background_cover_image_url\",\n",
    "                \"activities\",\n",
    "                \"volunteer_work\",\n",
    "            ]\n",
    "        }\n",
    "        if data.get(\"groups\"):\n",
    "            for group_dict in data.get(\"groups\"):\n",
    "                group_dict.pop(\"profile_pic_url\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c302fa",
   "metadata": {},
   "source": [
    "# Function that scrapes company's profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "4b240755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_company_data(linkedin_company_url):\n",
    "    api_key = \"2ZjHkun0Sq6KGa_o-4p7jg\"\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "\n",
    "    api_endpoint_1 = 'https://nubela.co/proxycurl/api/linkedin/company'\n",
    "    params_1 = {\n",
    "        'url': linkedin_company_url,\n",
    "        'use_cache': 'if-present',\n",
    "    }\n",
    "    response_1 = requests.get(api_endpoint_1, params=params_1, headers=headers)\n",
    "\n",
    "    if response_1.status_code == 200:\n",
    "        data = response_1.json()\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        api_endpoint_2 = 'https://nubela.co/proxycurl/api/linkedin/school'\n",
    "        params_2 = {\n",
    "            'url': linkedin_company_url,  # Utiliza la misma URL\n",
    "            'use_cache': 'if-present',\n",
    "        }\n",
    "        response_2 = requests.get(api_endpoint_2, params=params_2, headers=headers)\n",
    "\n",
    "        if response_2.status_code == 200:\n",
    "            data = response_2.json()\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Error al obtener datos de {linkedin_company_url} con la segunda opción.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b98b8",
   "metadata": {},
   "source": [
    "# Scraping and cleaning our dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df4f9c",
   "metadata": {},
   "source": [
    "## First dataframe: 2015-2016 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "61184812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We open our CSV, containing the full name and URL of the profile of our students, and we remove all those\\nrows that don't contain the URL of the student, because our Selenium code couldn't scrap it\""
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2015-2016.csv') \n",
    "\n",
    "df1 = df1[df1['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "'''We open our CSV, containing the full name and URL of the profile of our students, and we remove all those\n",
    "rows that don't contain the URL of the student, because our Selenium code couldn't scrap it'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "af017efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Datos escrapeados\"] = df1[\"Link perfil\"].apply(scrape_linkedin_profile2)\n",
    "\n",
    "# We apply our function to the row of our dataframe that contains all the URLs of our students\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "17c807e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df1.loc[df1['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df1 = df1.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "aba3bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "# We reset indexes and delete the column 'index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "039aa0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = pd.json_normalize(df1['Datos escrapeados']) \n",
    "\n",
    "# We create one column per key in our 'Datos escrapeados' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "b5e5522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([df1, normalized_data], axis=1) # We apply those columns to our df1 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "6a270690",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_eliminar = ['Nombre', 'Primer apellido', 'Segundo apellido', 'Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "\n",
    "df1 = df1.drop(columns=columnas_a_eliminar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "37730d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.explode('experiences')\n",
    "\n",
    "# We explode our lists in 'experiences' column, to have as many rows per student as experiences she/he has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "bb445aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "# We reset our indexes, to make it more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "f776d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(df1['experiences']) \n",
    "\n",
    "\n",
    "df1 = pd.concat([df1, normalized_data1], axis=1) # We apply those columns to our df1 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "21cdc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']                  \n",
    "                   \n",
    "df1 = df1.drop(cols_a_eliminar, axis=1)\n",
    "\n",
    "# We get rid of unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "dea8c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi1 = df1.copy() # We create a copy of our dataframe just in case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "78453be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi1 = copi1[copi1[\"company_linkedin_profile_url\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "d643753b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grado</th>\n",
       "      <th>Nombre completo</th>\n",
       "      <th>Link perfil</th>\n",
       "      <th>city</th>\n",
       "      <th>company</th>\n",
       "      <th>company_linkedin_profile_url</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>starts_at.day</th>\n",
       "      <th>starts_at.month</th>\n",
       "      <th>starts_at.year</th>\n",
       "      <th>ends_at.day</th>\n",
       "      <th>ends_at.month</th>\n",
       "      <th>ends_at.year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comunicación audiovisual y Publicidad</td>\n",
       "      <td>Antonio Cortés Ruiz</td>\n",
       "      <td>https://www.linkedin.com/in/antonio-cort%C3%A9...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>TBWA\\ España</td>\n",
       "      <td>https://es.linkedin.com/company/tbwa-spain</td>\n",
       "      <td>Junior Art Director/ Crearive</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comunicación audiovisual y Publicidad</td>\n",
       "      <td>Antonio Cortés Ruiz</td>\n",
       "      <td>https://www.linkedin.com/in/antonio-cort%C3%A9...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>ContrapuntoBBDO</td>\n",
       "      <td>https://es.linkedin.com/company/contra-punto-bbdo</td>\n",
       "      <td>Junior Art Director / Creative</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comunicación audiovisual y Publicidad</td>\n",
       "      <td>Antonio Cortés Ruiz</td>\n",
       "      <td>https://www.linkedin.com/in/antonio-cort%C3%A9...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>Publicis Groupe</td>\n",
       "      <td>https://es.linkedin.com/company/publicis-groupe</td>\n",
       "      <td>Junior Art Director / Creative</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comunicación audiovisual y Publicidad</td>\n",
       "      <td>Antonio Cortés Ruiz</td>\n",
       "      <td>https://www.linkedin.com/in/antonio-cort%C3%A9...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>Längstrump</td>\n",
       "      <td>https://es.linkedin.com/company/langstrump</td>\n",
       "      <td>Junior Art Director / Creative</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comunicación audiovisual y Publicidad</td>\n",
       "      <td>Antonio Cortés Ruiz</td>\n",
       "      <td>https://www.linkedin.com/in/antonio-cort%C3%A9...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>Grupo Boomerang TV</td>\n",
       "      <td>https://es.linkedin.com/company/boomerang-tv</td>\n",
       "      <td>Ayudante de Producción</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Comunicación digital</td>\n",
       "      <td>Alvaro Diaz Fernandez</td>\n",
       "      <td>https://www.linkedin.com/in/alvaro-diaz-fdez?m...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>Santander Global Operations</td>\n",
       "      <td>https://es.linkedin.com/company/santander-glob...</td>\n",
       "      <td>Digital Content Manager para Banco Santander (...</td>\n",
       "      <td>Madrid, Comunidad de Madrid, España</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Comunicación digital</td>\n",
       "      <td>Alvaro Diaz Fernandez</td>\n",
       "      <td>https://www.linkedin.com/in/alvaro-diaz-fdez?m...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>easylife conciliación</td>\n",
       "      <td>https://es.linkedin.com/company/easylife-conci...</td>\n",
       "      <td>Gestor de proyectos  - Carta de Beneficios</td>\n",
       "      <td>Madrid, Comunidad de Madrid, España</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>Comunicación digital</td>\n",
       "      <td>Alvaro Diaz Fernandez</td>\n",
       "      <td>https://www.linkedin.com/in/alvaro-diaz-fdez?m...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>easylife conciliación</td>\n",
       "      <td>https://es.linkedin.com/company/easylife-conci...</td>\n",
       "      <td>Especialista en comunicación corporativa y org...</td>\n",
       "      <td>Madrid y alrededores</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>Comunicación digital</td>\n",
       "      <td>Alvaro Diaz Fernandez</td>\n",
       "      <td>https://www.linkedin.com/in/alvaro-diaz-fdez?m...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>Grupo Trivium IC</td>\n",
       "      <td>https://es.linkedin.com/company/grupo-trivium-ic</td>\n",
       "      <td>Redactor de contenidos web</td>\n",
       "      <td>Gran Vía 33,  Madrid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Comunicación digital</td>\n",
       "      <td>Alvaro Diaz Fernandez</td>\n",
       "      <td>https://www.linkedin.com/in/alvaro-diaz-fdez?m...</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>Gabeiro Global Advisors</td>\n",
       "      <td>https://es.linkedin.com/company/gabeiro-global...</td>\n",
       "      <td>Community Manager</td>\n",
       "      <td>C/ Velazquez 71, Madrid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Grado        Nombre completo  \\\n",
       "0     Comunicación audiovisual y Publicidad    Antonio Cortés Ruiz   \n",
       "1     Comunicación audiovisual y Publicidad    Antonio Cortés Ruiz   \n",
       "2     Comunicación audiovisual y Publicidad    Antonio Cortés Ruiz   \n",
       "3     Comunicación audiovisual y Publicidad    Antonio Cortés Ruiz   \n",
       "4     Comunicación audiovisual y Publicidad    Antonio Cortés Ruiz   \n",
       "...                                     ...                    ...   \n",
       "1009                   Comunicación digital  Alvaro Diaz Fernandez   \n",
       "1010                   Comunicación digital  Alvaro Diaz Fernandez   \n",
       "1011                   Comunicación digital  Alvaro Diaz Fernandez   \n",
       "1012                   Comunicación digital  Alvaro Diaz Fernandez   \n",
       "1013                   Comunicación digital  Alvaro Diaz Fernandez   \n",
       "\n",
       "                                            Link perfil    city  \\\n",
       "0     https://www.linkedin.com/in/antonio-cort%C3%A9...  Madrid   \n",
       "1     https://www.linkedin.com/in/antonio-cort%C3%A9...  Madrid   \n",
       "2     https://www.linkedin.com/in/antonio-cort%C3%A9...  Madrid   \n",
       "3     https://www.linkedin.com/in/antonio-cort%C3%A9...  Madrid   \n",
       "4     https://www.linkedin.com/in/antonio-cort%C3%A9...  Madrid   \n",
       "...                                                 ...     ...   \n",
       "1009  https://www.linkedin.com/in/alvaro-diaz-fdez?m...  Madrid   \n",
       "1010  https://www.linkedin.com/in/alvaro-diaz-fdez?m...  Madrid   \n",
       "1011  https://www.linkedin.com/in/alvaro-diaz-fdez?m...  Madrid   \n",
       "1012  https://www.linkedin.com/in/alvaro-diaz-fdez?m...  Madrid   \n",
       "1013  https://www.linkedin.com/in/alvaro-diaz-fdez?m...  Madrid   \n",
       "\n",
       "                          company  \\\n",
       "0                    TBWA\\ España   \n",
       "1                 ContrapuntoBBDO   \n",
       "2                 Publicis Groupe   \n",
       "3                      Längstrump   \n",
       "4              Grupo Boomerang TV   \n",
       "...                           ...   \n",
       "1009  Santander Global Operations   \n",
       "1010        easylife conciliación   \n",
       "1011        easylife conciliación   \n",
       "1012             Grupo Trivium IC   \n",
       "1013      Gabeiro Global Advisors   \n",
       "\n",
       "                           company_linkedin_profile_url  \\\n",
       "0            https://es.linkedin.com/company/tbwa-spain   \n",
       "1     https://es.linkedin.com/company/contra-punto-bbdo   \n",
       "2       https://es.linkedin.com/company/publicis-groupe   \n",
       "3            https://es.linkedin.com/company/langstrump   \n",
       "4          https://es.linkedin.com/company/boomerang-tv   \n",
       "...                                                 ...   \n",
       "1009  https://es.linkedin.com/company/santander-glob...   \n",
       "1010  https://es.linkedin.com/company/easylife-conci...   \n",
       "1011  https://es.linkedin.com/company/easylife-conci...   \n",
       "1012   https://es.linkedin.com/company/grupo-trivium-ic   \n",
       "1013  https://es.linkedin.com/company/gabeiro-global...   \n",
       "\n",
       "                                                  title  \\\n",
       "0                         Junior Art Director/ Crearive   \n",
       "1                        Junior Art Director / Creative   \n",
       "2                        Junior Art Director / Creative   \n",
       "3                        Junior Art Director / Creative   \n",
       "4                                Ayudante de Producción   \n",
       "...                                                 ...   \n",
       "1009  Digital Content Manager para Banco Santander (...   \n",
       "1010         Gestor de proyectos  - Carta de Beneficios   \n",
       "1011  Especialista en comunicación corporativa y org...   \n",
       "1012                         Redactor de contenidos web   \n",
       "1013                                  Community Manager   \n",
       "\n",
       "                                 location  starts_at.day  starts_at.month  \\\n",
       "0                                    None            1.0             10.0   \n",
       "1                                    None            1.0              2.0   \n",
       "2                                  Madrid            1.0              6.0   \n",
       "3                                  Madrid            1.0              9.0   \n",
       "4                                  Madrid            1.0              4.0   \n",
       "...                                   ...            ...              ...   \n",
       "1009  Madrid, Comunidad de Madrid, España            1.0             10.0   \n",
       "1010  Madrid, Comunidad de Madrid, España            1.0             12.0   \n",
       "1011                 Madrid y alrededores            1.0             12.0   \n",
       "1012                 Gran Vía 33,  Madrid            1.0              9.0   \n",
       "1013              C/ Velazquez 71, Madrid            1.0              1.0   \n",
       "\n",
       "      starts_at.year  ends_at.day  ends_at.month  ends_at.year  \n",
       "0             2018.0         31.0            7.0        2019.0  \n",
       "1             2018.0         30.0            9.0        2018.0  \n",
       "2             2016.0         31.0           12.0        2016.0  \n",
       "3             2015.0         29.0            2.0        2016.0  \n",
       "4             2015.0         31.0            7.0        2015.0  \n",
       "...              ...          ...            ...           ...  \n",
       "1009          2021.0          NaN            NaN           NaN  \n",
       "1010          2020.0         30.0            9.0        2021.0  \n",
       "1011          2016.0         30.0            6.0        2021.0  \n",
       "1012          2015.0         31.0            1.0        2016.0  \n",
       "1013          2014.0         31.0           12.0        2014.0  \n",
       "\n",
       "[1014 rows x 14 columns]"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copi1 = copi1.reset_index()\n",
    "copi1 = copi1.drop('index', axis=1)\n",
    "\n",
    "copi1 \n",
    "\n",
    "# We reset indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "7802629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al obtener datos de https://www.linkedin.com/company/18505126/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/40655773/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/1675/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/1675/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/163929/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/2558499/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/2558499/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/2558499/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/15092653/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/9472/ con la segunda opción.\n",
      "Error al obtener datos de https://es.linkedin.com/company/private-tutor con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/1009/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/554396/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/10622779/ con la segunda opción.\n",
      "Error al obtener datos de https://www.linkedin.com/company/7389/ con la segunda opción.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[762], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m copi1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatos empresa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m copi1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_linkedin_profile_url\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(scrape_linkedin_company_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[752], line 10\u001b[0m, in \u001b[0;36mscrape_linkedin_company_data\u001b[0;34m(linkedin_company_url)\u001b[0m\n\u001b[1;32m      5\u001b[0m api_endpoint_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://nubela.co/proxycurl/api/linkedin/company\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m params_1 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: linkedin_company_url,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif-present\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m response_1 \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(api_endpoint_1, params\u001b[38;5;241m=\u001b[39mparams_1, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_1\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     13\u001b[0m     data \u001b[38;5;241m=\u001b[39m response_1\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    488\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    489\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    490\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    491\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    492\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    497\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    498\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "copi1[\"Datos empresa\"] = copi1[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "b9df72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "df2['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "df2['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "# If any of our ending columns have nulls, it means that that person is still working there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "024820f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "# We get rid of all our experiences whose start date is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "163cbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combines the columns of start date and end date into 'end date'\n",
    "df2['Fecha inicio'] = df2.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "df2['Fecha fin'] = df2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "# Calculates the duration of each experience in months\n",
    "df2['Duración (meses)'] = df2.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "# Replaces values in end date when its pertinent\n",
    "df2['Fecha fin'] = df2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "# Gets rid of unnecesary columns\n",
    "df2.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "0a94c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "# We rename some columns in spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "9afd1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reset_index()\n",
    "\n",
    "df2 = df2.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "1c5e5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "df2 = df2[new_order]\n",
    "\n",
    "# We finally change the orders of some columns to make them more comprehensible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "de186d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('/Users/davidfernandez/Desktop/clean/experience/2015-2016.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d434f4",
   "metadata": {},
   "source": [
    "## Second dataframe: 2016-2017 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "5ca48d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2016-2017.csv') \n",
    "\n",
    "df2 = df2[df2['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df2 = df2.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "ed8d7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"Datos escrapeados\"] = df2[\"Link perfil\"].apply(scrape_linkedin_profile2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "1b7061fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df2.loc[df2['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df2 = df2.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "f16244bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "dc8fc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi = copi.reset_index()\n",
    "copi = copi.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi['Datos escrapeados'])\n",
    "\n",
    "copi = pd.concat([copi, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Nombre', 'Primer apellido', 'Segundo apellido', 'Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi = copi.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi = copi.explode('experiences')\n",
    "\n",
    "copi = copi.reset_index()\n",
    "copi = copi.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi['experiences'])\n",
    "\n",
    "copi = pd.concat([copi, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi = copi.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "f0275d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi['Fecha inicio'] = copi.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi['Fecha fin'] = copi.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi['Duración (meses)'] = copi.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi['Fecha fin'] = copi.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi = copi.reset_index()\n",
    "copi = copi.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi = copi[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "4e8350e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi.to_csv('/Users/davidfernandez/Desktop/clean/experience/2016-2017.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9e3d3",
   "metadata": {},
   "source": [
    "## Third dataframe: 2017-2018 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "7cc5f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2017-2018.csv') \n",
    "\n",
    "df3 = df3[df3['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df3 = df3.reset_index()\n",
    "\n",
    "df3 = df3.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "74806c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Datos escrapeados\"] = df3[\"Link perfil\"].apply(scrape_linkedin_profile2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "8cf5628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df3.loc[df3['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df3 = df3.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "b9222037",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "18d7f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi2['Datos escrapeados'])\n",
    "\n",
    "copi2 = pd.concat([copi2, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi2 = copi2.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi2 = copi2.explode('experiences')\n",
    "\n",
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi2['experiences'])\n",
    "\n",
    "copi2 = pd.concat([copi2, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi2 = copi2.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "2cfb7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi2['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi2['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi2.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi2['Fecha inicio'] = copi2.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi2['Fecha fin'] = copi2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi2['Duración (meses)'] = copi2.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi2['Fecha fin'] = copi2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi2.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi2.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi2 = copi2[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "5ce4c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2.to_csv('/Users/davidfernandez/Desktop/clean/experience/2017-2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f143dbd2",
   "metadata": {},
   "source": [
    "## Fifth dataframe: 2019-2020 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "c0cae716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2019-2020.csv')\n",
    "\n",
    "df5 = df5[df5['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df5 = df5.reset_index()\n",
    "\n",
    "df5 = df5.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "3fb1685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[\"Datos escrapeados\"] = df5[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "f670edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df5.loc[df5['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df5 = df5.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "d6fe7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3 = df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "a5d8976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi3['Datos escrapeados'])\n",
    "\n",
    "copi3 = pd.concat([copi3, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi3 = copi3.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi3 = copi3.explode('experiences')\n",
    "\n",
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi3['experiences'])\n",
    "\n",
    "copi3 = pd.concat([copi3, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi3 = copi3.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "55acf36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi3['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi3['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi3.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi3['Fecha inicio'] = copi3.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi3['Fecha fin'] = copi3.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi3['Duración (meses)'] = copi3.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi3['Fecha fin'] = copi3.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi3.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi3.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi3 = copi3[new_order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "123a1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3.to_csv('/Users/davidfernandez/Desktop/clean/experience/2019-2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8bdcc0",
   "metadata": {},
   "source": [
    "## Sixth dataframe: 2020-2021 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "e1b8855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2020-2021.csv')\n",
    "\n",
    "df6 = df6[df6['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df6 = df6.reset_index()\n",
    "\n",
    "df6 = df6.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "12621f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6[\"Datos escrapeados\"] = df6[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "ac7af116",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df6.loc[df6['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df6 = df6.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "1835f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4 = df6.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "1dbcb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4 = copi4.reset_index()\n",
    "copi4 = copi4.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi4['Datos escrapeados'])\n",
    "\n",
    "copi4 = pd.concat([copi4, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi4 = copi4.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi4 = copi4.explode('experiences')\n",
    "\n",
    "copi4 = copi4.reset_index()\n",
    "copi4 = copi4.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi4['experiences'])\n",
    "\n",
    "copi4 = pd.concat([copi4, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi4 = copi4.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "28b1b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi4['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi4['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi4.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi4['Fecha inicio'] = copi4.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi4['Fecha fin'] = copi4.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi4['Duración (meses)'] = copi4.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi4['Fecha fin'] = copi4.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi4.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi4.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi4 = copi4.reset_index()\n",
    "copi4 = copi4.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi4 = copi4[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "8fe9b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4.to_csv('/Users/davidfernandez/Desktop/clean/experience/2020-2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2a61c",
   "metadata": {},
   "source": [
    "## Seventh dataframe: 2021-2022 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "f8c825e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2021-2022.csv')\n",
    "\n",
    "df7 = df7[df7['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df7 = df7.reset_index()\n",
    "\n",
    "df7 = df7.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "1b24e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7[\"Datos escrapeados\"] = df7[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "7105df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df7.loc[df7['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df7 = df7.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "23116928",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5 = df7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "c29076de",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi5['Datos escrapeados'])\n",
    "\n",
    "copi5 = pd.concat([copi5, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi5 = copi5.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi5 = copi5.explode('experiences')\n",
    "\n",
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi5['experiences'])\n",
    "\n",
    "copi5 = pd.concat([copi5, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi5 = copi5.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "beff92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi5['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi5['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi5.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi5['Fecha inicio'] = copi5.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi5['Fecha fin'] = copi5.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi5['Duración (meses)'] = copi5.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi5['Fecha fin'] = copi5.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi5.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi5.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi5 = copi5[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "4e465502",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5.to_csv('/Users/davidfernandez/Desktop/clean/experience/2021-2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118d005",
   "metadata": {},
   "source": [
    "## Eight dataframe: 2022-2023 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "14cd2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2022-2023.csv')\n",
    "\n",
    "df8 = df8[df8['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df8 = df8.reset_index()\n",
    "\n",
    "df8 = df8.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "a442fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8[\"Datos escrapeados\"] = df8[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "ce238a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df8.loc[df8['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df8 = df8.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "e7f41fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6 = df8.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "dbeabc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi6['Datos escrapeados'])\n",
    "\n",
    "copi6 = pd.concat([copi6, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi6 = copi6.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi6 = copi6.explode('experiences')\n",
    "\n",
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi6['experiences'])\n",
    "\n",
    "copi6 = pd.concat([copi6, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi6 = copi6.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "b8ad6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi6['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi6['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi6.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi6['Fecha inicio'] = copi6.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi6['Fecha fin'] = copi6.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi6['Duración (meses)'] = copi6.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi6['Fecha fin'] = copi6.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi6.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi6.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi6 = copi6[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "449b96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6.to_csv('/Users/davidfernandez/Desktop/clean/experience/2022-2023.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

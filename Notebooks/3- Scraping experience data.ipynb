{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f261a0cb",
   "metadata": {},
   "source": [
    "# Scrapping LinkedIn profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "2289c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from requests.exceptions import RequestException\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a82640",
   "metadata": {},
   "source": [
    "# Function that scrapes LinkedIn profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "44a8f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_profile2(linkedin_profile_url: str):\n",
    "    api_key = \"2ZjHkun0Sq6KGa_o-4p7jg\"\n",
    "    header_dic = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    api_endpoint = \"https://nubela.co/proxycurl/api/v2/linkedin\"\n",
    "\n",
    "    response = requests.get(\n",
    "        api_endpoint, params={\"url\": linkedin_profile_url}, headers=header_dic\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        data = {\n",
    "            k: v\n",
    "            for k, v in data.items()\n",
    "            if v not in ([], \"\", \"\", None)\n",
    "            and k\n",
    "            not in [\n",
    "                \"people_also_viewed\",\n",
    "                \"recommendations\",\n",
    "                \"similarly_named_profiles\",\n",
    "                \"articles\",\n",
    "                \"background_cover_image_url\",\n",
    "                \"activities\",\n",
    "                \"volunteer_work\",\n",
    "            ]\n",
    "        }\n",
    "        if data.get(\"groups\"):\n",
    "            for group_dict in data.get(\"groups\"):\n",
    "                group_dict.pop(\"profile_pic_url\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4df1d2",
   "metadata": {},
   "source": [
    "# Function that scrapes company's profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "7da328e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_company_data(linkedin_company_url):\n",
    "    api_key = \"2ZjHkun0Sq6KGa_o-4p7jg\"\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "\n",
    "    api_endpoint_1 = 'https://nubela.co/proxycurl/api/linkedin/company'\n",
    "    params_1 = {\n",
    "        'url': linkedin_company_url,\n",
    "        'use_cache': 'if-present',\n",
    "    }\n",
    "    response_1 = requests.get(api_endpoint_1, params=params_1, headers=headers)\n",
    "\n",
    "    if response_1.status_code == 200:\n",
    "        data = response_1.json()\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        api_endpoint_2 = 'https://nubela.co/proxycurl/api/linkedin/school'\n",
    "        params_2 = {\n",
    "            'url': linkedin_company_url,  # Utiliza la misma URL\n",
    "            'use_cache': 'if-present',\n",
    "        }\n",
    "        response_2 = requests.get(api_endpoint_2, params=params_2, headers=headers)\n",
    "\n",
    "        if response_2.status_code == 200:\n",
    "            data = response_2.json()\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Error al obtener datos de {linkedin_company_url} con la segunda opción.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb28c4",
   "metadata": {},
   "source": [
    "# Scraping and cleaning our dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144297b",
   "metadata": {},
   "source": [
    "## First dataframe: 2015-2016 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "728fe185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We open our CSV, containing the full name and URL of the profile of our students, and we remove all those\\nrows that don't contain the URL of the student, because our Selenium code couldn't scrap it\""
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2015-2016.csv') \n",
    "\n",
    "df1 = df1[df1['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "'''We open our CSV, containing the full name and URL of the profile of our students, and we remove all those\n",
    "rows that don't contain the URL of the student, because our Selenium code couldn't scrap it'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "cd9cb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Datos escrapeados\"] = df1[\"Link perfil\"].apply(scrape_linkedin_profile2)\n",
    "\n",
    "# We apply our function to the row of our dataframe that contains all the URLs of our students\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "a2b6831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df1.loc[df1['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df1 = df1.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "27c16e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "# We reset indexes and delete the column 'index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "ad3972b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = pd.json_normalize(df1['Datos escrapeados']) \n",
    "\n",
    "# We create one column per key in our 'Datos escrapeados' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "e033ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([df1, normalized_data], axis=1) # We apply those columns to our df1 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "640193f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_eliminar = ['Nombre', 'Primer apellido', 'Segundo apellido', 'Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "\n",
    "df1 = df1.drop(columns=columnas_a_eliminar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "0f7bfac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.explode('experiences')\n",
    "\n",
    "# We explode our lists in 'experiences' column, to have as many rows per student as experiences she/he has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "81968af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "# We reset our indexes, to make it more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "4c7ec9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(df1['experiences']) \n",
    "\n",
    "\n",
    "df1 = pd.concat([df1, normalized_data1], axis=1) # We apply those columns to our df1 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "562bacd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Grado', 'Nombre completo', 'Link perfil', 'city', 'experiences',\n",
       "       'company', 'company_linkedin_profile_url', 'title', 'description',\n",
       "       'location', 'logo_url', 'starts_at.day', 'starts_at.month',\n",
       "       'starts_at.year', 'ends_at.day', 'ends_at.month', 'ends_at.year',\n",
       "       'ends_at', 'starts_at'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "9235f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']                  \n",
    "                   \n",
    "df1 = df1.drop(cols_a_eliminar, axis=1)\n",
    "\n",
    "# We get rid of unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "9669eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi1 = df1.copy() # We create a copy of our dataframe just in case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79736e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi1[\"Datos empresa\"] = copi1[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)\n",
    "\n",
    "# We scrape all the data from each company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "292857dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii1 = copi1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "e3e68b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(copii1['Datos empresa']) \n",
    "\n",
    "\n",
    "copii1 = pd.concat([copii1, normalized_data1], axis=1) # We apply those columns to our df1 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "d7ceb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop1 = ['linkedin_internal_id', 'specialities', 'Datos empresa', 'company_type', 'description', 'website', 'company_size', 'company_size_on_linkedin', 'founded_year', 'locations', 'name', 'tagline', 'universal_name_id', 'profile_pic_url', 'background_cover_image_url', 'search_id', 'similar_companies', 'affiliated_companies', 'updates', 'follower_count', 'acquisitions', 'exit_data', 'extra', 'funding_data', 'categories', 'hq.country', 'hq.city', 'hq.postal_code', 'hq.line_1', 'hq.is_hq', 'hq.state', 'hq']\n",
    "\n",
    "\n",
    "copii1.drop(to_drop1, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "7141a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii1['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copii1['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copii1['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "# If any of our ending columns have nulls, it means that that person is still working there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "9e26a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii1.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "# We get rid of all our experiences whose start date is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "725e71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combines the columns of start date and end date into 'end date'\n",
    "copii1['Fecha inicio'] = copii1.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copii1['Fecha fin'] = copii1.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "copii1['Duración (meses)'] = copii1.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "copii1['Fecha fin'] = copii1.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "copii1.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "71db590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii1.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad', 'industry': 'Sector'}, inplace=True)\n",
    "\n",
    "# We rename some columns in spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "id": "7859581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Sector', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copii1 = copii1[new_order]\n",
    "\n",
    "# We finally change the orders of some columns to make them more comprehensible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "id": "0c0d0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = copii1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "7af60445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('/Users/davidfernandez/Desktop/clean/experience/2015-2016.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ac136",
   "metadata": {},
   "source": [
    "## Second dataframe: 2016-2017 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "0033fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2016-2017.csv') \n",
    "\n",
    "df2 = df2[df2['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df2 = df2.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "7c2a6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"Datos escrapeados\"] = df2[\"Link perfil\"].apply(scrape_linkedin_profile2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "99f6c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df2.loc[df2['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df2 = df2.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "e4f275a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2 = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "6b67495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi2['Datos escrapeados'])\n",
    "\n",
    "copi2 = pd.concat([copi2, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Nombre', 'Primer apellido', 'Segundo apellido', 'Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi2 = copi2.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi2 = copi2.explode('experiences')\n",
    "\n",
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi2['experiences'])\n",
    "\n",
    "copi2 = pd.concat([copi2, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']\n",
    "\n",
    "copi2 = copi2.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "0586572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi2['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi2['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi2.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi2['Fecha inicio'] = copi2.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi2['Fecha fin'] = copi2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi2['Duración (meses)'] = copi2.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi2['Fecha fin'] = copi2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi2.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi2.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2[\"company_linkedin_profile_url\"] = copi2[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "4f802b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii2 = copi2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "44f762ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(copii2['company_linkedin_profile_url']) \n",
    "\n",
    "\n",
    "copii2 = pd.concat([copii2, normalized_data1], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "e20e6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop1 = ['linkedin_internal_id', 'specialities', 'company_linkedin_profile_url', 'company_type', 'description', 'website', 'company_size', 'company_size_on_linkedin', 'founded_year', 'locations', 'name', 'tagline', 'universal_name_id', 'profile_pic_url', 'background_cover_image_url', 'search_id', 'similar_companies', 'affiliated_companies', 'updates', 'follower_count', 'acquisitions', 'exit_data', 'extra', 'funding_data', 'categories', 'hq.country', 'hq.city', 'hq.postal_code', 'hq.line_1', 'hq.is_hq', 'hq.state', 'hq']\n",
    "\n",
    "\n",
    "copii2.drop(to_drop1, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "b8c8111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii2.rename(columns={'industry': 'Sector'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "2ffac5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Sector', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copii2 = copii2[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "cc4b157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii2.to_csv('/Users/davidfernandez/Desktop/clean/experience/2016-2017.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42c227",
   "metadata": {},
   "source": [
    "## Third dataframe: 2017-2018 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "649d8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2017-2018.csv') \n",
    "\n",
    "df3 = df3[df3['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df3 = df3.reset_index()\n",
    "\n",
    "df3 = df3.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "f56a16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Datos escrapeados\"] = df3[\"Link perfil\"].apply(scrape_linkedin_profile2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "8d6e33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df3.loc[df3['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df3 = df3.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "4f1a70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "d4e0a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi3['Datos escrapeados'])\n",
    "\n",
    "copi3 = pd.concat([copi3, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi3 = copi3.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi3 = copi3.explode('experiences')\n",
    "\n",
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi3['experiences'])\n",
    "\n",
    "copi3 = pd.concat([copi3, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']\n",
    "\n",
    "copi3 = copi3.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "0d0d5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi3['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi3['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi3.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi3['Fecha inicio'] = copi3.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi3['Fecha fin'] = copi3.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi3['Duración (meses)'] = copi3.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi3['Fecha fin'] = copi3.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi3.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi3.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37dd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3[\"Datos empresa\"] = copi3[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "1d761e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii3 = copi3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "7bdb95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(copii3['Datos empresa']) \n",
    "\n",
    "copii3 = pd.concat([copii3, normalized_data1], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "b969ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop1 = ['linkedin_internal_id', 'specialities', 'Datos empresa', 'company_type', 'description', 'website', 'company_size', 'company_size_on_linkedin', 'founded_year', 'locations', 'name', 'tagline', 'universal_name_id', 'profile_pic_url', 'background_cover_image_url', 'search_id', 'similar_companies', 'affiliated_companies', 'updates', 'follower_count', 'acquisitions', 'exit_data', 'extra', 'funding_data', 'categories', 'hq.country', 'hq.city', 'hq.postal_code', 'hq.line_1', 'hq.is_hq', 'hq.state', 'hq']\n",
    "\n",
    "\n",
    "copii3.drop(to_drop1, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "b24d9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii3.rename(columns={'industry': 'Sector'}, inplace=True)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Sector', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copii3 = copii3[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "id": "2293dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii3.to_csv('/Users/davidfernandez/Desktop/clean/experience/2017-2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bb36e",
   "metadata": {},
   "source": [
    "## Fifth dataframe: 2019-2020 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "c5dcec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2019-2020.csv')\n",
    "\n",
    "df5 = df5[df5['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df5 = df5.reset_index()\n",
    "\n",
    "df5 = df5.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "c41d32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[\"Datos escrapeados\"] = df5[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "98ff792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df5.loc[df5['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df5 = df5.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "5715fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5 = df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "5409fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi5['Datos escrapeados'])\n",
    "\n",
    "copi5 = pd.concat([copi5, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi5 = copi5.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi5 = copi5.explode('experiences')\n",
    "\n",
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi5['experiences'])\n",
    "\n",
    "copi5 = pd.concat([copi5, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']\n",
    "\n",
    "copi5 = copi5.drop(cols_a_eliminar, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "07fa6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi5['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi5['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi5.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi5['Fecha inicio'] = copi5.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi5['Fecha fin'] = copi5.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi5['Duración (meses)'] = copi5.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi5['Fecha fin'] = copi5.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi5.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi5.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23646966",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5[\"Datos empresa\"] = copi5[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "b4ef94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii5 = copi5.copy()\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copii5['Datos empresa']) \n",
    "\n",
    "copii5 = pd.concat([copii5, normalized_data1], axis=1) \n",
    "\n",
    "to_drop1 = ['linkedin_internal_id', 'specialities', 'Datos empresa', 'company_type', 'description', 'website', 'company_size', 'company_size_on_linkedin', 'founded_year', 'locations', 'name', 'tagline', 'universal_name_id', 'profile_pic_url', 'background_cover_image_url', 'search_id', 'similar_companies', 'affiliated_companies', 'updates', 'follower_count', 'acquisitions', 'exit_data', 'extra', 'funding_data', 'categories', 'hq.country', 'hq.city', 'hq.postal_code', 'hq.line_1', 'hq.is_hq', 'hq.state', 'hq']\n",
    "\n",
    "copii5.drop(to_drop1, axis=1, inplace=True)\n",
    "\n",
    "copii5.rename(columns={'industry': 'Sector'}, inplace=True)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Sector', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copii5 = copii5[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "acd0b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "copii5.to_csv('/Users/davidfernandez/Desktop/clean/experience/2019-2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e4883",
   "metadata": {},
   "source": [
    "## Sixth dataframe: 2020-2021 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "914c4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2020-2021.csv')\n",
    "\n",
    "df6 = df6[df6['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df6 = df6.reset_index()\n",
    "\n",
    "df6 = df6.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "fc27f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6[\"Datos escrapeados\"] = df6[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "ac16daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df6.loc[df6['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df6 = df6.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "id": "ef3f914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6 = df6.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "20f965e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi6['Datos escrapeados'])\n",
    "\n",
    "copi6 = pd.concat([copi6, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi6 = copi6.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi6 = copi6.explode('experiences')\n",
    "\n",
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi6['experiences'])\n",
    "\n",
    "copi6 = pd.concat([copi6, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']\n",
    "\n",
    "copi6 = copi6.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "d9663373",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi6['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi6['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi6.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi6['Fecha inicio'] = copi6.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi6['Fecha fin'] = copi6.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi6['Duración (meses)'] = copi6.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi6['Fecha fin'] = copi6.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi6.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi6.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6[\"Datos empresa\"] = copi6[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "fa25a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(copi6['Datos empresa']) \n",
    "\n",
    "copi6 = pd.concat([copi6, normalized_data1], axis=1) \n",
    "\n",
    "to_drop1 = ['linkedin_internal_id', 'specialities', 'Datos empresa', 'company_type', 'description', 'website', 'company_size', 'company_size_on_linkedin', 'founded_year', 'locations', 'name', 'tagline', 'universal_name_id', 'profile_pic_url', 'background_cover_image_url', 'search_id', 'similar_companies', 'affiliated_companies', 'updates', 'follower_count', 'acquisitions', 'exit_data', 'extra', 'funding_data', 'categories', 'hq.country', 'hq.city', 'hq.postal_code', 'hq.line_1', 'hq.is_hq', 'hq.state', 'hq']\n",
    "\n",
    "copi6.drop(to_drop1, axis=1, inplace=True)\n",
    "\n",
    "copi6.rename(columns={'industry': 'Sector'}, inplace=True)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Sector', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi6 = copi6[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "id": "fd5e8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6.to_csv('/Users/davidfernandez/Desktop/clean/experience/2020-2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebdfb6",
   "metadata": {},
   "source": [
    "## Seventh dataframe: 2021-2022 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "5640dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2021-2022.csv')\n",
    "\n",
    "df7 = df7[df7['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df7 = df7.reset_index()\n",
    "\n",
    "df7 = df7.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "c39669b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7[\"Datos escrapeados\"] = df7[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "f1121c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df7.loc[df7['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df7 = df7.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "f3086301",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi7 = df7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "adfc88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi7 = copi7.reset_index()\n",
    "copi7 = copi7.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi7['Datos escrapeados'])\n",
    "\n",
    "copi7 = pd.concat([copi7, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi7 = copi7.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi7 = copi7.explode('experiences')\n",
    "\n",
    "copi7 = copi7.reset_index()\n",
    "copi7 = copi7.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi7['experiences'])\n",
    "\n",
    "copi7 = pd.concat([copi7, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']\n",
    "\n",
    "copi7 = copi7.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "id": "c08c3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi7['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi7['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi7['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi7.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi7['Fecha inicio'] = copi7.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi7['Fecha fin'] = copi7.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi7['Duración (meses)'] = copi7.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi7['Fecha fin'] = copi7.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi7.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi7.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi7 = copi7.reset_index()\n",
    "copi7 = copi7.drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7bd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi7[\"Datos empresa\"] = copi7[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "99303861",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(copi7['Datos empresa']) \n",
    "\n",
    "copi7 = pd.concat([copi7, normalized_data1], axis=1) \n",
    "\n",
    "to_drop1 = ['linkedin_internal_id', 'specialities', 'Datos empresa', 'company_type', 'description', 'website', 'company_size', 'company_size_on_linkedin', 'founded_year', 'locations', 'name', 'tagline', 'universal_name_id', 'profile_pic_url', 'background_cover_image_url', 'search_id', 'similar_companies', 'affiliated_companies', 'updates', 'follower_count', 'acquisitions', 'exit_data', 'extra', 'funding_data', 'categories', 'hq.country', 'hq.city', 'hq.postal_code', 'hq.line_1', 'hq.is_hq', 'hq.state', 'hq']\n",
    "\n",
    "copi7.drop(to_drop1, axis=1, inplace=True)\n",
    "\n",
    "copi7.rename(columns={'industry': 'Sector'}, inplace=True)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Sector', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi7 = copi7[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "id": "928cff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi7.to_csv('/Users/davidfernandez/Desktop/clean/experience/2021-2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635e4ac",
   "metadata": {},
   "source": [
    "## Eight dataframe: 2022-2023 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "5c867714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2022-2023.csv')\n",
    "\n",
    "df8 = df8[df8['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df8 = df8.reset_index()\n",
    "\n",
    "df8 = df8.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "428d08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8[\"Datos escrapeados\"] = df8[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "674e385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df8.loc[df8['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df8 = df8.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "dd400bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi8 = df8.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "8ef4fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi8 = copi8.reset_index()\n",
    "copi8 = copi8.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi8['Datos escrapeados'])\n",
    "\n",
    "copi8 = pd.concat([copi8, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi8 = copi8.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi8 = copi8.explode('experiences')\n",
    "\n",
    "copi8 = copi8.reset_index()\n",
    "copi8 = copi8.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi8['experiences'])\n",
    "\n",
    "copi8 = pd.concat([copi8, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'experiences']\n",
    "\n",
    "copi8 = copi8.drop(cols_a_eliminar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "ea0b96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi8['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi8['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi8['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi8.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi8['Fecha inicio'] = copi8.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi8['Fecha fin'] = copi8.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi8['Duración (meses)'] = copi8.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi8['Fecha fin'] = copi8.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi8.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi8.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi8 = copi8.reset_index()\n",
    "copi8 = copi8.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi8[\"Datos empresa\"] = copi8[\"company_linkedin_profile_url\"].apply(scrape_linkedin_company_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "id": "0b500031",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(copi8['Datos empresa']) \n",
    "\n",
    "copi8 = pd.concat([copi8, normalized_data1], axis=1) \n",
    "\n",
    "to_drop1 = ['linkedin_internal_id', 'specialities', 'Datos empresa', 'company_type', 'description', 'website', 'company_size', 'company_size_on_linkedin', 'founded_year', 'locations', 'name', 'tagline', 'universal_name_id', 'profile_pic_url', 'background_cover_image_url', 'search_id', 'similar_companies', 'affiliated_companies', 'updates', 'follower_count', 'acquisitions', 'exit_data', 'extra', 'funding_data', 'categories', 'hq.country', 'hq.city', 'hq.postal_code', 'hq.line_1', 'hq.is_hq', 'hq.state', 'hq']\n",
    "\n",
    "copi8.drop(to_drop1, axis=1, inplace=True)\n",
    "\n",
    "copi8.rename(columns={'industry': 'Sector'}, inplace=True)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Sector', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi8 = copi8[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "f9e0eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi8.to_csv('/Users/davidfernandez/Desktop/clean/experience/2022-2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d40fd7",
   "metadata": {},
   "source": [
    "# Some extra actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "id": "8786a9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/y46jwd8j5nn1qp81l6_znfkh0000gn/T/ipykernel_76198/891368230.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  copii2['Fecha graduación'] = pd.to_datetime('2017-06-01')\n"
     ]
    }
   ],
   "source": [
    "df1['Fecha graduación'] = pd.to_datetime('2016-06-01')\n",
    "copii2['Fecha graduación'] = pd.to_datetime('2017-06-01')\n",
    "copii3['Fecha graduación'] = pd.to_datetime('2018-06-01')\n",
    "copii5['Fecha graduación'] = pd.to_datetime('2020-06-01')\n",
    "copi6['Fecha graduación'] = pd.to_datetime('2021-06-01')\n",
    "copi7['Fecha graduación'] = pd.to_datetime('2022-06-01')\n",
    "copi8['Fecha graduación'] = pd.to_datetime('2023-06-01')\n",
    "\n",
    "\n",
    "# We create an extra column with the graduation date of each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "id": "28f448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dataframes = [df11, copii2, copii3, copii5, copi6, copi7, copi8]  \n",
    "\n",
    "finaldf = pd.concat(lista_dataframes, ignore_index=True)\n",
    "\n",
    "# We concatenate all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "id": "c27f65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_a_area = {\n",
    "    'Advertising Services': 'Marketing & Advertising',\n",
    "    'Marketing & Advertising': 'Marketing & Advertising',\n",
    "    'Media Production': 'Media & Entertainment',\n",
    "    'Broadcast Media': 'Media & Entertainment',\n",
    "    'Newspapers': 'Media & Entertainment',\n",
    "    'Online Media': 'Media & Entertainment',\n",
    "    'Broadcast Media Production and Distribution': 'Media & Entertainment',\n",
    "    'Aviation and Aerospace Component Manufacturing': 'Aerospace & Aviation',\n",
    "    'Retail': 'Retail',\n",
    "    'Spectator Sports': 'Sports & Recreation',\n",
    "    'Higher Education': 'Education',\n",
    "    'Perangkat Lunak Komputer': 'Computer Software',\n",
    "    'Pharmaceutical Manufacturing': 'Pharmaceuticals',\n",
    "    'Airlines and Aviation': 'Aerospace & Aviation',\n",
    "    'Staffing and Recruiting': 'Human Resources',\n",
    "    'Architecture and Planning': 'Architecture & Planning',\n",
    "    'Apparel & Fashion': 'Fashion',\n",
    "    'Hospitality': 'Hospitality & Tourism',\n",
    "    'Writing and Editing': 'Writing & Editing',\n",
    "    'Computer Software': 'Computer Software',\n",
    "    'Retail Apparel and Fashion': 'Fashion',\n",
    "    'Retail Luxury Goods and Jewelry': 'Fashion',\n",
    "    'Education Management': 'Education',\n",
    "    'Luxury Goods & Jewelry': 'Fashion',\n",
    "    'Pharmaceuticals': 'Pharmaceuticals',\n",
    "    'Hospitals and Health Care': 'Healthcare',\n",
    "    'Software Development': 'Computer Software',\n",
    "    'Events Services': 'Events & Entertainment',\n",
    "    'International Affairs': 'Government & Politics',\n",
    "    'Entertainment Providers': 'Media & Entertainment',\n",
    "    'IT Services and IT Consulting': 'Information Technology',\n",
    "    'Law Practice': 'Legal Services',\n",
    "    'Education Administration Programs': 'Education',\n",
    "    'Primary/Secondary Education': 'Education',\n",
    "    'Wholesale Building Materials': 'Construction',\n",
    "    'Textile Manufacturing': 'Textiles',\n",
    "    'Banking': 'Finance',\n",
    "    'Artists and Writers': 'Arts & Creativity',\n",
    "    'Legal Services': 'Legal Services',\n",
    "    'Civic and Social Organizations': 'Non-Profit & NGOs',\n",
    "    'Fine Art': 'Arts & Creativity',\n",
    "    'Textiles': 'Textiles',\n",
    "    'Business Consulting and Services': 'Business Consulting',\n",
    "    'Marketing and Advertising': 'Marketing & Advertising',\n",
    "    'Research Services': 'Research & Development',\n",
    "    'Professional Training & Coaching': 'Training & Coaching',\n",
    "    'Libraries': 'Libraries',\n",
    "    'Outsourcing/Offshoring': 'Outsourcing & Offshoring',\n",
    "    'E-learning': 'Education',\n",
    "    'Newspaper Publishing': 'Media & Entertainment',\n",
    "    'Utilities': 'Utilities',\n",
    "    'Government Administration': 'Government & Politics',\n",
    "    'Civic & Social Organization': 'Non-Profit & NGOs',\n",
    "    'Computer & Network Security': 'Information Technology',\n",
    "    'Public Policy Offices': 'Government & Politics',\n",
    "    'Animation': 'Media & Entertainment',\n",
    "    'Entertainment': 'Media & Entertainment',\n",
    "    'Animation and Post-production': 'Media & Entertainment',\n",
    "    'Travel Arrangements': 'Hospitality & Tourism',\n",
    "    'Financial Services': 'Finance',\n",
    "    'Public Relations & Communications': 'Marketing & Advertising',\n",
    "    'Health, Wellness and Fitness': 'Health & Wellness',\n",
    "    'Internet': 'Information Technology',\n",
    "    'Publishing': 'Media & Entertainment',\n",
    "    'Market Research': 'Research & Development',\n",
    "    'Musicians': 'Music & Sound',\n",
    "    'Personal Care Product Manufacturing': 'Beauty & Personal Care',\n",
    "    'Medical Devices': 'Medical Devices',\n",
    "    'Food and Beverage Services': 'Food & Beverage',\n",
    "    'Non-profit Organizations': 'Non-Profit & NGOs',\n",
    "    'Human Resources Services': 'Human Resources',\n",
    "    'Professional Training and Coaching': 'Training & Coaching',\n",
    "    'Non-profit Organization Management': 'Non-Profit & NGOs',\n",
    "    'Transportation/Trucking/Railroad': 'Transportation & Logistics',\n",
    "    'Think Tanks': 'Government & Politics',\n",
    "    'Information Technology & Services': 'Information Technology',\n",
    "    'Security and Investigations': 'Security & Investigations',\n",
    "    'Telecommunications': 'Telecommunications',\n",
    "    'Restaurants': 'Food & Beverage',\n",
    "    'Internet Publishing': 'Media & Entertainment',\n",
    "    'Veterinary': 'Veterinary',\n",
    "    'Chemical Manufacturing': 'Chemical Manufacturing',\n",
    "    'Consumer Services': 'Consumer Services',\n",
    "    'Arts and Crafts': 'Arts & Creativity',\n",
    "    'Semiconductor Manufacturing': 'Electronics & Semiconductors',\n",
    "    'Movies, Videos, and Sound': 'Media & Entertainment',\n",
    "    'Renewable Energy Semiconductor Manufacturing': 'Electronics & Semiconductors',\n",
    "    'Marketing Services': 'Marketing & Advertising',\n",
    "    'Automotive': 'Automotive',\n",
    "    'Technology, Information and Internet': 'Information Technology',\n",
    "    'Public Relations and Communications Services': 'Marketing & Advertising',\n",
    "    'Environmental Services': 'Environment & Sustainability',\n",
    "    'Retail Art Supplies': 'Retail',\n",
    "    'Book and Periodical Publishing': 'Media & Entertainment',\n",
    "    'Public Relations and Communications': 'Marketing & Advertising',\n",
    "    'Online Audio and Video Media': 'Media & Entertainment',\n",
    "    'Photography': 'Photography',\n",
    "    'Research': 'Research & Development',\n",
    "    'International Trade and Development': 'International Trade & Development',\n",
    "    'Management Consulting': 'Management Consulting',\n",
    "    'Insurance': 'Insurance',\n",
    "    'Information Services': 'Information Technology',\n",
    "    'Architecture & Planning': 'Architecture & Planning',\n",
    "    'Commercial Real Estate': 'Real Estate',\n",
    "    'Individual & Family Services': 'Social Services',\n",
    "    'Armed Forces': 'Military'}\n",
    "\n",
    "# We create a dictionary that groups each sector into a general area\n",
    "\n",
    "finaldf['Sector'] = finaldf['Sector'].map(sector_a_area)\n",
    "\n",
    "# And we map each sector with its area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "f446af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades_generales = {\n",
    "    'Madrid': 'Madrid',\n",
    "    'madrid': 'Madrid',\n",
    "    'Greater Madrid Metropolitan Area': 'Madrid',\n",
    "    'Barcelona': 'Barcelona',\n",
    "    'Comunidad de Madrid': 'Madrid',\n",
    "    'Majadahonda': 'Madrid',\n",
    "    'Pozuelo de Alarcón': 'Madrid',\n",
    "    'Moralzarzal': 'Madrid',\n",
    "    'Palma': 'Palma',\n",
    "    'Angoulême': 'Angoulême',\n",
    "    'London': 'Londres',\n",
    "    'Madrid y alrededores': 'Madrid',\n",
    "    'GijÃ³n': 'Gijón',\n",
    "    'Alcobendas': 'Madrid',\n",
    "    'Basigo': 'Otras',\n",
    "    'España': 'Otras',\n",
    "    'london': 'Londres',\n",
    "    'Suances': 'Suances',\n",
    "    'brussels': 'Bruselas',\n",
    "    'fuenlabrada': 'Madrid',\n",
    "    'Germania': 'Otras',\n",
    "    'Palencia': 'Palencia',\n",
    "    'Alicante/Alacant': 'Alicante',\n",
    "    'Berlin': 'Berlín',\n",
    "    'Stockholm': 'Estocolmo',\n",
    "    'Ciudad Real': 'Ciudad Real',\n",
    "    'Bayern': 'Bayern',\n",
    "    'Medina del Campo': 'Medina del Campo',\n",
    "    'Santa Cruz de Tenerife y alrededores': 'Santa Cruz de Tenerife',\n",
    "    'lisbon': 'Lisboa',\n",
    "    'Amsterdam': 'Ámsterdam',\n",
    "    'Córdoba': 'Córdoba',\n",
    "    'Greater Ciudad Real Metropolitan Area': 'Ciudad Real',\n",
    "    'Monforte de Lemos': 'Monforte de Lemos',\n",
    "    'Mexico': 'México',\n",
    "    'Greater Paris Metropolitan Region': 'París',\n",
    "    'Tokyo': 'Tokio',\n",
    "    'barcelona': 'Barcelona',\n",
    "    'München': 'München',\n",
    "    'Vigo': 'Vigo',\n",
    "    'Las Rozas de Madrid': 'Madrid',\n",
    "    'Salt Lake City': 'Salt Lake City',\n",
    "    'Móstoles': 'Madrid',\n",
    "    'Almería y alrededores': 'Almería',\n",
    "    'León': 'León',\n",
    "    'Burgos': 'Burgos',\n",
    "    'Seville': 'Sevilla',\n",
    "    'Oviedo': 'Oviedo',\n",
    "    'copenhagen': 'Copenhague',\n",
    "    'pozuelo de alarcón': 'Madrid',\n",
    "    'Torrelodones': 'Madrid',\n",
    "    'Alcalá de Henares': 'Madrid',\n",
    "    'Tres Cantos': 'Madrid',\n",
    "    'Bilbao-Bilbo': 'Bilbao',\n",
    "    'Vitoria-Gasteiz': 'Vitoria-Gasteiz',\n",
    "    'majadahonda': 'Madrid',\n",
    "    'Brussels': 'Bruselas',\n",
    "    'Greater Valladolid Metropolitan Area': 'Valladolid',\n",
    "    'Liverpool': 'Liverpool',\n",
    "    'Bari': 'Bari',\n",
    "    'Paris': 'París',\n",
    "    'Santander': 'Santander',\n",
    "    'PietÃ\\xa0': 'Pietà',\n",
    "    'Logroño': 'Logroño',\n",
    "    'Hamilton': 'Hamilton',\n",
    "    'Brussels Metropolitan Area': 'Bruselas',\n",
    "    'alcalá de henares': 'Madrid',\n",
    "    'Arroyo': 'Arroyo',\n",
    "    'Alcorcón': 'Madrid',\n",
    "    'Manzanares el Real': 'Madrid',\n",
    "    'Galapagar': 'Madrid',\n",
    "    'Bremen': 'Bremen',\n",
    "    'Granada': 'Granada',\n",
    "    'Rome': 'Roma',\n",
    "    'Lisbon': 'Lisboa',\n",
    "    'Milano': 'Milán',\n",
    "    'San Bartolomé': 'San Bartolomé',\n",
    "    'San Jose': 'San José',\n",
    "    'Sanlúcar de Barrameda': 'Sanlúcar de Barrameda',\n",
    "    'Lisboa': 'Lisboa',\n",
    "    'San Sebastián de los Reyes': 'San Sebastián de los Reyes',\n",
    "    'Utrecht': 'Utrecht',\n",
    "    'Villaviciosa de Odón': 'Madrid',\n",
    "    'marbella': 'Marbella',\n",
    "    'Zaragoza': 'Zaragoza',\n",
    "    'Siviglia': 'Sevilla',\n",
    "    'San Sebastián': 'San Sebastián',\n",
    "    'Sevilla': 'Sevilla',\n",
    "    'Dublin': 'Dublín'}\n",
    "\n",
    "# We create a dictionary that groups each location into a general city\n",
    "\n",
    "\n",
    "finaldf['Ciudad'] = finaldf['Ciudad'].map(ciudades_generales)\n",
    "\n",
    "# And we map each location with its city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf['Tipo de Grado'] = finaldf['Grado'].apply(lambda x: 'Doble grado' if ' y ' in x else 'Grado individual')\n",
    "\n",
    "# We create a new column depending if it's a double degree or an individual degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "id": "1de4a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapeo_grados = {\n",
    "    'Comunicación audiovisual y Publicidad': 'Comunicación audiovisual y Publicidad',\n",
    "    'Comunicación audiovisual y Periodismo': 'Comunicación audiovisual y Periodismo',\n",
    "    'Comunicación audiovisual': 'Comunicación audiovisual',\n",
    "    'Publicidad y Comunicación audiovisual': 'Publicidad y Comunicación audiovisual',\n",
    "    'Publicidad': 'Publicidad',\n",
    "    'Publicidad y Periodismo': 'Publicidad y Periodismo',\n",
    "    'Historia Arte y Historia': 'Historia y Historia del arte',\n",
    "    'Periodismo y Humanidades': 'Periodismo y Humanidades',\n",
    "    'Historia': 'Historia',\n",
    "    'Periodismo': 'Periodismo',\n",
    "    'Periodismo y Comunicación audiovisual': 'Periodismo y Comunicación audiovisual',\n",
    "    'Periodismo y Publicidad': 'Periodismo y Publicidad',\n",
    "    'Comunicación digital': 'Comunicación digital',\n",
    "    'Humanidades': 'Humanidades',\n",
    "    'Gr. Comunicación Digital': 'Comunicación digital',\n",
    "    'Gr. Comunicación Digital y Publicidad': 'Comunicación digital y Publicidad',\n",
    "    'Gr. Comunicación Digital y Comunicación audiovisual': 'Comunicación digital y Comunicación audiovisual',\n",
    "    'Educación infantil y Educación primaria': 'Educación infantil y Educación primaria',\n",
    "    'Periodismo y Gr. Comunicación Digital': 'Periodismo y Comunicación digital',\n",
    "    'Gr. Comunicación Digital y Periodismo': 'Comunicación digital y Periodismo',\n",
    "    'Historia y Historia Arte': 'Historia y Historia del arte',\n",
    "    'Comunicación digital y Publicidad': 'Comunicación digital y Publicidad',\n",
    "    'Periodismo y Comunicación digital': 'Periodismo y Comunicación digital',\n",
    "    'Historia Arte': 'Historia del Arte',\n",
    "    'Publicidad y Humanidades': 'Publicidad y Humanidades',\n",
    "    'Comunicación audiovisual y Comunicación digital': 'Comunicación audiovisual y Comunicación digital',\n",
    "    'Publicidad y Comunicación digital': 'Publicidad y Comunicación digital',\n",
    "    'Humanidades y periodismo': 'Humanidades y Periodismo',\n",
    "    'Comunicación audiovisual y publicidad': 'Comunicación audiovisual y Publicidad',\n",
    "    'Historia y Historia del arte': 'Historia y Historia del arte',\n",
    "    'Historia del arte': 'Historia del Arte',\n",
    "    'Comunicación digital y periodismo': 'Comunicación digital y Periodismo',\n",
    "    'Comunicación digital y publicidad': 'Comunicación digital y Publicidad',\n",
    "    'Historia y historia del arte': 'Historia y Historia del Arte',\n",
    "    'Historia y periodismo': 'Historia y Periodismo',\n",
    "    'Humanidades y comunicación audiovisual': 'Humanidades y Comunicación audiovisual',\n",
    "    'Periodismo y comunicación audiovisual': 'Periodismo y Comunicación audiovisual',\n",
    "    'Periodismo y publicidad': 'Periodismo y Publicidad',\n",
    "    'Comunicación digital y comunicación audiovisual': 'Comunicación digital y Comunicación audiovisual',\n",
    "    'Humanidades y comunicación digital': 'Humanidades y Comunicación digital',\n",
    "    'Historia y Historia del arte y Historia e historia del arte': 'Historia y Historia del Arte',\n",
    "    'Historia del arte y Historia e historia del arte': 'Historia y Historia del Arte',\n",
    "    'Humanidades y periodismo y Periodismo': 'Humanidades y Periodismo',\n",
    "    'Humanidades y comunicación digital y Comunicación digital': 'Humanidades y Comunicación digital',\n",
    "    'Periodismo y Publicidad y Periodismo y publicidad': 'Periodismo y Publicidad',\n",
    "    'Periodismo y Comunicación audiovisual y Periodismo y comunicación audiovisual': 'Periodismo y Comunicación audiovisual',\n",
    "    'Periodismo y Comunicación digital y periodismo': 'Periodismo y Comunicación digital',\n",
    "    'Periodismo y Periodismo y comunicación audiovisual': 'Periodismo y Comunicación audiovisual',\n",
    "    'Comunicación audiovisual y Publicidad y Comunicación audiovisual y publicidad': 'Comunicación audiovisual y Publicidad',\n",
    "    'Comunicación audiovisual y Comunicación digital y Comunicación digital y comunicación audiovisual': 'Comunicación audiovisual y Comunicación digital',\n",
    "    'Publicidad y Comunicación digital y publicidad': 'Publicidad y Comunicación digital',\n",
    "    'Publicidad y Periodismo y publicidad': 'Publicidad y Periodismo',\n",
    "    'Publicidad y Comunicación digital y Comunicación digital y publicidad': 'Publicidad y Comunicación digital',\n",
    "    'Comunicación digital y Comunicación digital y publicidad': 'Comunicación digital y Publicidad'}\n",
    "\n",
    "\n",
    "# Aplicar el mapeo a la columna 'Grado' en tu DataFrame\n",
    "finaldf['Grado'] = finaldf['Grado'].map(mapeo_grados)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "id": "2391d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.dropna(subset=['Grado'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "id": "c9705b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = finaldf.reset_index()\n",
    "\n",
    "finaldf = finaldf.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "id": "72dda703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinar_rama(grado):\n",
    "\n",
    "    if 'Humanidades' in grado or 'Historia' in grado:\n",
    "        return 'Humanidades'\n",
    "    \n",
    "    if 'Comunicación audiovisual' in grado or 'Publicidad' in grado or 'Comunicación digital' in grado or 'Periodismo' in grado:\n",
    "        return 'Ciencias de la comunicación'\n",
    "    \n",
    "    if ' y ' in grado:\n",
    "        grados = grado.split(' y ')\n",
    "        if 'Humanidades' in grados[0] or 'Historia' in grados[0]:\n",
    "            return 'Humanidades'\n",
    "    \n",
    "    if ' y ' in grado:\n",
    "        grados = grado.split(' y ')\n",
    "        if 'Comunicación audiovisual' in grados[0] or 'Publicidad' in grados[0] or 'Comunicación digital' in grados[0] or 'Periodismo' in grados[0]:\n",
    "            return 'Ciencias de la comunicación'\n",
    "    \n",
    "    return None\n",
    "\n",
    "finaldf['Rama'] = finaldf['Grado'].apply(determinar_rama)\n",
    "\n",
    "# We create a function that categorizes our degree in 'Humanities' or 'Communication sciences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "id": "85e4a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = finaldf[finaldf['Grado'] != 'Educación infantil y Educación primaria']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "id": "5f67407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.to_csv('/Users/davidfernandez/Desktop/clean/experience/FINAL.csv', index=False) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

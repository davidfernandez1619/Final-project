{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f7f658",
   "metadata": {},
   "source": [
    "# Scrapping LinkedIn profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "15f16917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from requests.exceptions import RequestException\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dea6da",
   "metadata": {},
   "source": [
    "# Function that scrapes LinkedIn profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "95527224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_profile(linkedin_profile_url: str):\n",
    "    \n",
    "    api_key = \"2ZjHkun0Sq6KGa_o-4p7jg\"\n",
    "    header_dic = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    api_endpoint = \"https://nubela.co/proxycurl/api/v2/linkedin\"\n",
    "\n",
    "    response = requests.get(\n",
    "        api_endpoint, params={\"url\": linkedin_profile_url}, headers=header_dic\n",
    "    )\n",
    "\n",
    "    data = response.json()\n",
    "    data = {\n",
    "        k: v\n",
    "        for k, v in data.items()\n",
    "        if v not in ([], \"\", \"\", None)\n",
    "        and k\n",
    "        not in [\n",
    "            \"people_also_viewed\",\n",
    "            \"recommendations\",\n",
    "            \"similarly_named_profiles\",\n",
    "            \"articles\",\n",
    "            \"background_cover_image_url\",\n",
    "            \"activities\",\n",
    "            \"volunteer_work\",\n",
    "        ]\n",
    "    }\n",
    "    if data.get(\"groups\"):\n",
    "        for group_dict in data.get(\"groups\"):\n",
    "            group_dict.pop(\"profile_pic_url\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# This function manually scrapes a LinkedIn profile, taking the person's URL as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "05e15bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_profile2(linkedin_profile_url: str):\n",
    "    api_key = \"2ZjHkun0Sq6KGa_o-4p7jg\"\n",
    "    header_dic = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    api_endpoint = \"https://nubela.co/proxycurl/api/v2/linkedin\"\n",
    "\n",
    "    response = requests.get(\n",
    "        api_endpoint, params={\"url\": linkedin_profile_url}, headers=header_dic\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        data = {\n",
    "            k: v\n",
    "            for k, v in data.items()\n",
    "            if v not in ([], \"\", \"\", None)\n",
    "            and k\n",
    "            not in [\n",
    "                \"people_also_viewed\",\n",
    "                \"recommendations\",\n",
    "                \"similarly_named_profiles\",\n",
    "                \"articles\",\n",
    "                \"background_cover_image_url\",\n",
    "                \"activities\",\n",
    "                \"volunteer_work\",\n",
    "            ]\n",
    "        }\n",
    "        if data.get(\"groups\"):\n",
    "            for group_dict in data.get(\"groups\"):\n",
    "                group_dict.pop(\"profile_pic_url\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f82ac1",
   "metadata": {},
   "source": [
    "# Scraping and cleaning our dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a522a68",
   "metadata": {},
   "source": [
    "## First dataframe: 2015-2016 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "e5f3e82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We open our CSV, containing the full name and URL of the profile of our students, and we remove all those\\nrows that don't contain the URL of the student, because our Selenium code couldn't scrap it\""
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2015-2016.csv') \n",
    "\n",
    "df1 = df1[df1['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "'''We open our CSV, containing the full name and URL of the profile of our students, and we remove all those\n",
    "rows that don't contain the URL of the student, because our Selenium code couldn't scrap it'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "ec229aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Datos escrapeados\"] = df1[\"Link perfil\"].apply(scrape_linkedin_profile2)\n",
    "\n",
    "# We apply our function to the row of our dataframe that contains all the URLs of our students\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "17333b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df1.loc[df1['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df1 = df1.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "acf5a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "# We reset indexes and delete the column 'index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "39af3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = pd.json_normalize(df1['Datos escrapeados']) \n",
    "\n",
    "# We create one column per key in our 'Datos escrapeados' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "f4a529e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([df1, normalized_data], axis=1) # We apply those columns to our copi2 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "a54f1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_eliminar = ['Nombre', 'Primer apellido', 'Segundo apellido', 'Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "\n",
    "df1 = df1.drop(columns=columnas_a_eliminar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "0c11dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.explode('experiences')\n",
    "\n",
    "# We explode our lists in 'experiences' column, to have as many rows per student as experiences she/he has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "e9a694ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()\n",
    "\n",
    "df1 = df1.drop('index', axis=1)\n",
    "\n",
    "# We reset our indexes, to make it more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "c6d7ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data1 = pd.json_normalize(df1['experiences']) \n",
    "\n",
    "\n",
    "df1 = pd.concat([df1, normalized_data1], axis=1) # We apply those columns to our df1 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "bafb956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']                  \n",
    "                   \n",
    "df1 = df1.drop(cols_a_eliminar, axis=1)\n",
    "\n",
    "\n",
    "# We get rid of unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "fd472551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "f5602cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "df2['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "df2['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "# If any of our ending columns have nulls, it means that that person is still working there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "9886c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "# We get rid of all our experiences whose start date is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "b35e6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combines the columns of start date and end date into 'end date'\n",
    "df2['Fecha inicio'] = df2.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "df2['Fecha fin'] = df2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "# Calculates the duration of each experience in months\n",
    "df2['Duración (meses)'] = df2.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "# Replaces values in end date when its pertinent\n",
    "df2['Fecha fin'] = df2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "# Gets rid of unnecesary columns\n",
    "df2.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "d4713674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "# We rename some columns in spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "83cc288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reset_index()\n",
    "\n",
    "df2 = df2.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "bd6caa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "df2 = df2[new_order]\n",
    "\n",
    "# We finally change the orders of some columns to make them more comprehensible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "7cd58011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('/Users/davidfernandez/Desktop/clean/experience/2015-2016.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200e123",
   "metadata": {},
   "source": [
    "## Second dataframe: 2016-2017 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "af85eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2016-2017.csv') \n",
    "\n",
    "df2 = df2[df2['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df2 = df2.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "21e5686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"Datos escrapeados\"] = df2[\"Link perfil\"].apply(scrape_linkedin_profile2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "e1fca870",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df2.loc[df2['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df2 = df2.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "72d7b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "41a26938",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi = copi.reset_index()\n",
    "copi = copi.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi['Datos escrapeados'])\n",
    "\n",
    "copi = pd.concat([copi, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Nombre', 'Primer apellido', 'Segundo apellido', 'Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi = copi.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi = copi.explode('experiences')\n",
    "\n",
    "copi = copi.reset_index()\n",
    "copi = copi.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi['experiences'])\n",
    "\n",
    "copi = pd.concat([copi, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi = copi.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "41b0b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi['Fecha inicio'] = copi.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi['Fecha fin'] = copi.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi['Duración (meses)'] = copi.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi['Fecha fin'] = copi.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi = copi.reset_index()\n",
    "copi = copi.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi = copi[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "99846282",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi.to_csv('/Users/davidfernandez/Desktop/clean/experience/2016-2017.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bcad0",
   "metadata": {},
   "source": [
    "## Third dataframe: 2017-2018 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "9cc65d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2017-2018.csv') \n",
    "\n",
    "df3 = df3[df3['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df3 = df3.reset_index()\n",
    "\n",
    "df3 = df3.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "b61e3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Datos escrapeados\"] = df3[\"Link perfil\"].apply(scrape_linkedin_profile2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "8ad241da",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df3.loc[df3['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df3 = df3.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "3f95c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "a58f3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi2['Datos escrapeados'])\n",
    "\n",
    "copi2 = pd.concat([copi2, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi2 = copi2.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi2 = copi2.explode('experiences')\n",
    "\n",
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi2['experiences'])\n",
    "\n",
    "copi2 = pd.concat([copi2, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi2 = copi2.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "3ea35185",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi2['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi2['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi2.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi2['Fecha inicio'] = copi2.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi2['Fecha fin'] = copi2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi2['Duración (meses)'] = copi2.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi2['Fecha fin'] = copi2.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi2.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi2.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi2 = copi2.reset_index()\n",
    "copi2 = copi2.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi2 = copi2[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "53e4505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi2.to_csv('/Users/davidfernandez/Desktop/clean/experience/2017-2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9aead",
   "metadata": {},
   "source": [
    "## Fifth dataframe: 2019-2020 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "9ac4f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2019-2020.csv')\n",
    "\n",
    "df5 = df5[df5['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df5 = df5.reset_index()\n",
    "\n",
    "df5 = df5.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "2782396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[\"Datos escrapeados\"] = df5[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "73db7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df5.loc[df5['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df5 = df5.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "b77ac7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3 = df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "c6f1f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi3['Datos escrapeados'])\n",
    "\n",
    "copi3 = pd.concat([copi3, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi3 = copi3.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi3 = copi3.explode('experiences')\n",
    "\n",
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi3['experiences'])\n",
    "\n",
    "copi3 = pd.concat([copi3, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi3 = copi3.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "b679e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi3['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi3['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi3.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi3['Fecha inicio'] = copi3.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi3['Fecha fin'] = copi3.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi3['Duración (meses)'] = copi3.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi3['Fecha fin'] = copi3.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi3.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi3.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi3 = copi3.reset_index()\n",
    "copi3 = copi3.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi3 = copi3[new_order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "bd212adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi3.to_csv('/Users/davidfernandez/Desktop/clean/experience/2019-2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02afabc2",
   "metadata": {},
   "source": [
    "## Sixth dataframe: 2020-2021 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "2522e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2020-2021.csv')\n",
    "\n",
    "df6 = df6[df6['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df6 = df6.reset_index()\n",
    "\n",
    "df6 = df6.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "263039be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6[\"Datos escrapeados\"] = df6[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "ff1fc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df6.loc[df6['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df6 = df6.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "e161dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4 = df6.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "385f7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4 = copi4.reset_index()\n",
    "copi4 = copi4.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi4['Datos escrapeados'])\n",
    "\n",
    "copi4 = pd.concat([copi4, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi4 = copi4.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi4 = copi4.explode('experiences')\n",
    "\n",
    "copi4 = copi4.reset_index()\n",
    "copi4 = copi4.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi4['experiences'])\n",
    "\n",
    "copi4 = pd.concat([copi4, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi4 = copi4.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "41865fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi4['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi4['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi4.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi4['Fecha inicio'] = copi4.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi4['Fecha fin'] = copi4.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi4['Duración (meses)'] = copi4.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi4['Fecha fin'] = copi4.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi4.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi4.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi4 = copi4.reset_index()\n",
    "copi4 = copi4.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi4 = copi4[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "5de5e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi4.to_csv('/Users/davidfernandez/Desktop/clean/experience/2020-2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4b6d4",
   "metadata": {},
   "source": [
    "## Seventh dataframe: 2021-2022 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "48ce33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2021-2022.csv')\n",
    "\n",
    "df7 = df7[df7['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df7 = df7.reset_index()\n",
    "\n",
    "df7 = df7.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "a92a211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7[\"Datos escrapeados\"] = df7[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "63f13f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df7.loc[df7['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df7 = df7.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "c02f74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5 = df7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "583b338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi5['Datos escrapeados'])\n",
    "\n",
    "copi5 = pd.concat([copi5, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Email personal', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi5 = copi5.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi5 = copi5.explode('experiences')\n",
    "\n",
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi5['experiences'])\n",
    "\n",
    "copi5 = pd.concat([copi5, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi5 = copi5.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "f2348ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi5['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi5['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi5.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi5['Fecha inicio'] = copi5.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi5['Fecha fin'] = copi5.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi5['Duración (meses)'] = copi5.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi5['Fecha fin'] = copi5.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi5.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi5.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi5 = copi5.reset_index()\n",
    "copi5 = copi5.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi5 = copi5[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "3faf1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi5.to_csv('/Users/davidfernandez/Desktop/clean/experience/2021-2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046def9",
   "metadata": {},
   "source": [
    "## Eight dataframe: 2022-2023 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "e8d72ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.read_csv('/Users/davidfernandez/Desktop/clean/scrapped/2022-2023.csv')\n",
    "\n",
    "df8 = df8[df8['Link perfil'] != 'Link no existente']\n",
    "\n",
    "df8 = df8.reset_index()\n",
    "\n",
    "df8 = df8.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "6c3ac301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8[\"Datos escrapeados\"] = df8[\"Link perfil\"].apply(scrape_linkedin_profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "21a87b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_a_buscar = {'code': 404, 'description': 'Person profile does not exist', 'name': 'Not Found'}\n",
    "\n",
    "\n",
    "filas_filtradas = df8.loc[df8['Datos escrapeados'] == valor_a_buscar]\n",
    "\n",
    "# We identify those students whose data we couldn't scrap\n",
    "\n",
    "indices_a_eliminar = filas_filtradas.index\n",
    "\n",
    "df8 = df8.drop(indices_a_eliminar)\n",
    "\n",
    "# And we take them out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "393bcb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6 = df8.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "2db91239",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "normalized_data = pd.json_normalize(copi6['Datos escrapeados'])\n",
    "\n",
    "copi6 = pd.concat([copi6, normalized_data], axis=1)\n",
    "\n",
    "columnas_a_eliminar = ['Email universitario', 'Datos escrapeados', 'public_identifier', 'profile_pic_url', 'first_name', 'last_name', 'full_name', 'headline', 'country', 'languages', 'education', 'occupation', 'connections', 'country_full_name', 'follower_count', 'summary', 'state', 'accomplishment_honors_awards', 'accomplishment_courses', 'accomplishment_projects', 'groups', 'accomplishment_publications', 'certifications', 'accomplishment_organisations', 'accomplishment_test_scores']\n",
    "\n",
    "copi6 = copi6.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "copi6 = copi6.explode('experiences')\n",
    "\n",
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "normalized_data1 = pd.json_normalize(copi6['experiences'])\n",
    "\n",
    "copi6 = pd.concat([copi6, normalized_data1], axis=1)\n",
    "\n",
    "cols_a_eliminar = ['description', 'logo_url', 'ends_at', 'starts_at', 'company_linkedin_profile_url', 'experiences']\n",
    "\n",
    "copi6 = copi6.drop(cols_a_eliminar, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "e0e0e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6['ends_at.day'].fillna('actualidad', inplace=True)\n",
    "copi6['ends_at.month'].fillna('actualidad', inplace=True)\n",
    "copi6['ends_at.year'].fillna('actualidad', inplace=True)\n",
    "\n",
    "copi6.dropna(subset=['starts_at.day', 'starts_at.month', 'starts_at.year'], inplace=True)\n",
    "\n",
    "copi6['Fecha inicio'] = copi6.apply(lambda x: pd.to_datetime(f\"{int(x['starts_at.year'])}-{int(x['starts_at.month'])}-01\"), axis=1)\n",
    "copi6['Fecha fin'] = copi6.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else pd.to_datetime(f\"{int(x['ends_at.year'])}-{int(x['ends_at.month'])}-01\"), axis=1)\n",
    "\n",
    "copi6['Duración (meses)'] = copi6.apply(lambda x: (datetime.datetime.now() - x['Fecha inicio']).days // 30 if x['ends_at.year'] == 'actualidad' else (x['Fecha fin'] - x['Fecha inicio']).days // 30, axis=1)\n",
    "\n",
    "copi6['Fecha fin'] = copi6.apply(lambda x: 'actualidad' if x['ends_at.year'] == 'actualidad' else x['Fecha fin'], axis=1)\n",
    "\n",
    "copi6.drop(['location', 'starts_at.year', 'starts_at.day', 'starts_at.month', 'ends_at.day', 'ends_at.month', 'ends_at.year'], axis=1, inplace=True)\n",
    "\n",
    "copi6.rename(columns={'company': 'Empresa', 'title': 'Puesto', 'city': 'Ciudad'}, inplace=True)\n",
    "\n",
    "copi6 = copi6.reset_index()\n",
    "copi6 = copi6.drop('index', axis=1)\n",
    "\n",
    "new_order = ['Grado', 'Nombre completo', 'Link perfil', 'Puesto', 'Empresa', 'Ciudad', 'Fecha inicio', 'Fecha fin', 'Duración (meses)']\n",
    "\n",
    "copi6 = copi6[new_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "730667f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "copi6.to_csv('/Users/davidfernandez/Desktop/clean/experience/2022-2023.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
